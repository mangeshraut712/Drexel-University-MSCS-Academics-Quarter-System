{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment group 4: Machine learning and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module A _(49 pts)_ Exploring dimensionality reduction through image classification\n",
    "__Data.__ For this module we'll be working with the MNIST dataset of 28 x 28 black and white pixel-images of hand-written numbers, which was put together some folks at NYU, Google Labs, and Microsoft Research:\n",
    "\n",
    "- http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "For simplicity, we won't be access these data from the producers' site, since it's presented in a compressed format that requires special code (there's a [python module](https://pypi.org/project/python-mnist/), if you're interested).\n",
    "\n",
    "The MNIST dataset has become extremely important to the ML community over the years as a standard dataset for image classification that has a very clear 'true' label set (unlike others, e.g., like sentiment classification in __Module B__). For ML in general, it's very important to have datasets over which the community can compare results, hence compare algorithms without the uncertainty of data variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A1.__ _(5 pts_) To load the MNIST data, we're actually going to be able to use `sklearn`! Since the data have become such a standard, and `sklearn` is _the_ standard ML library it has an `sklearn.datasets.fetch_openml()` method that we can use to download datasets:\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html\n",
    "\n",
    "So, using this method, load the MNIST data as `mnist`, setting `name='mnist_784'`, `version=1`, and `cache=True` as arguments. Note: the latter two arguments 1) standardize our verson and 2) store the downloaded data on your local machine. If you're ever curious, the data should wind up in `'~/scikit_learn_data/'`.\n",
    "\n",
    "Once you've loaded `mnist`, inspect its `.data` attribute. In particular, print and discuss the size and type (number of features and their values) of a a single record `mnist.data[i]` for some `i`, and describe in the response box below how these data appear represent a black and white image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784,),\n",
       " array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   9.,  80., 207., 255.,\n",
       "        254., 254., 254.,  97.,  80.,  80.,  44.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  39., 158.,\n",
       "        158., 158., 168., 253., 253., 253., 253., 253., 253., 253., 253.,\n",
       "        253., 210.,  38.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0., 226., 253., 253., 253., 253., 253., 253.,\n",
       "        253., 253., 253., 253., 253., 253., 253., 253., 241., 146.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 139.,\n",
       "        253., 253., 253., 238., 113., 215., 253., 253., 253., 253., 253.,\n",
       "        253., 253., 253., 253., 210.,  43.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,  39.,  34.,  34.,  34.,  30.,   0.,\n",
       "         31., 148.,  34., 204., 235., 253., 253., 253., 253., 253., 236.,\n",
       "         64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         91.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  35., 199.,\n",
       "        253., 253., 253., 253., 244.,  81.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         11.,  33., 202., 202., 216., 253., 253., 253., 253., 241.,  89.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,  11., 167., 253., 253., 253., 253.,\n",
       "        253., 253., 253., 238.,  82.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         27., 253., 253., 253., 253., 253., 253., 253., 253.,  96.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,  18., 201., 253., 253., 253.,\n",
       "        253., 253., 253., 253., 230.,  49.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,  36.,  87.,  87.,  87., 248., 253., 253., 253., 253.,\n",
       "        138.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   7., 152., 253., 253., 253., 250.,  59.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  62., 238., 253.,\n",
       "        253., 253.,  60.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,  32., 233., 253., 253., 150.,   6.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  37., 203., 253.,\n",
       "        253., 253., 138.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,  66., 211., 211., 211.,  59.,  36.,  36.,  21.,\n",
       "         26.,  36., 151., 222., 253., 253., 253., 253., 138.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 253.,\n",
       "        253., 253., 253., 253., 253., 195., 215., 253., 253., 253., 253.,\n",
       "        253., 253., 157.,  77.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,  80., 253., 253., 253., 253., 253., 253.,\n",
       "        253., 253., 253., 253., 253., 253., 237., 235.,  40.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  49.,\n",
       "        156., 247., 253., 253., 253., 253., 253., 253., 253., 253., 159.,\n",
       "        156.,  16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0., 116., 253., 253., 253.,\n",
       "        253., 253., 126.,  78.,  78.,   3.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data[27].shape, mnist.data[27]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A2.__ _(7 pts)_ Spoiler alert! Each image record is represented as an integer `np.array` of size `(784,)` (you still have to show that above). Each integer represents a pixel, with shade of gray encoded as an intensity from 0&ndash;255, i.e., white&ndash;black. Notice how 28 divides 784 evenly? That's because the rows of the image have been 'stacked' into a single row of data!\n",
    "\n",
    "Since we're going to want to visualize these images, e.g., to check and see why our algorithm fails where. But in order to do this, we're going to need to build a helper function that pre-processes a given record into it's representation as a `(28,28)` matrix of rows. In particular, write a function called `unstack(x)` that accepts any image/record `x` and outputs an `np.array()` of size `(28, 28)` containing the pixel values such that the top row holds pixels 0&ndash;27, the second contains pixels 28&ndash;55, etc.\n",
    "\n",
    "[Hint. `numpy.array`s have a `.reshape()` method that can do the unstacking for you, check it out!]\n",
    "\n",
    "When complete, print the result from application to the same record as observed in __A1__. Can you see any shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 13 25 100 122 7 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 11 175 253 252 71 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 178 252 252 252 252 253 252 252 252 252 252 252 252 59 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 20 254 255 48 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 216 253 60 0 0 0 0 212 255 81 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 24 209 254 254 254 171 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 144 253 145 12 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 54 251 254 254 254 248 74 5 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 76 252 253 223 37 0 48 174 252 252 242 214 253 199 31 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 29 255 254 109 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 77 247 252 248 106 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 9 31 82 137 203 203 212 254 254 254 254 251 223 223 127 52 33 0 0 0\n",
      "0 0 0 0 0 0 0 0 26 123 254 253 203 156 253 200 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 68 255 13 0 0 0 0 0 0 77 240 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 41 179 232 84 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 33 228 252 252 252 157 4 0 0 0 0 0\n",
      "0 0 0 0 0 0 63 236 251 253 251 251 251 251 253 251 188 94 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 8 206 223 11 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 99 248 253 119 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def unstack(x):\n",
    "    pixels = np.array(mnist.data[i], dtype='uint8')\n",
    "    return pixels.reshape((28, 28))\n",
    "\n",
    "for i in range(unstack(mnist.data[27]).shape[0]):\n",
    "    print(\" \".join(map(str,unstack(mnist.data[27])[i,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A3.__ _(3 pts)_ Now that we've got the ability to unstack our images, let's visualize them. In particular, write a function called `show_pixels(px)` that \n",
    "\n",
    "1. accepts an unstacked collection of pixels, `px`, and\n",
    "2. utilizes `plt.imshow()` on `px` with an appropriate `cmap` argument to express the pictures.\n",
    "\n",
    "When complete, exhibit your function's output on your same chosen example from __A1&ndash;A3__. You should definitely be able to tell what number the picture represents now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAFpCAYAAABajglzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAEYtJREFUeJzt3WuIXHWax/HfLx3Fu7SkNwYTN65EQRZXlzYujIirznh54w1lVMasDEZhBI0X1gs48cXihdEsqCgRZVxQR0Fdo4Zdgwru6CK2EpJonDVoZBJi0l7QDIIx8dkXfdTW6U6ff1edrnqqvx8I6a5+UvWUlXw9qa46cUQIANDdZnR6AQDAxIg1ACRArAEgAWINAAkQawBIgFgDQALEGgASINYAkACxBoAEiDUAJDBzKm9s1qxZMX/+/Km8SQDoahs3btQnn3ziieamNNbz58/X0NDQVN4kAHS1wcHBWnMtPQ1i+3Tbf7K9wfYNrVwXAGB8k4617T5J90k6Q9JRki60fVS7FgMA/KCVI+uFkjZExAcRsUPSHySd1Z61AACjtRLrQyT9edTnm6rLfsT2YttDtoeGh4dbuDkAmL4af+leRCyPiMGIGBwYGGj65gCgJ7US682S5o36fG51GQCgzVqJ9ZuSFtg+zPaekn4paUV71gIAjDbp11lHxE7bV0r6b0l9kh6OiHfathkA4HstvSkmIlZKWtmmXQAA4+DcIACQALEGgASINQAkQKwBIAFiDQAJEGsASIBYA0ACxBoAEiDWAJAAsQaABIg1ACRArAEgAWINAAkQawBIgFgDQALEGgASINYAkACxBoAEiDUAJECsASABYg0ACRBrAEiAWANAAsQaABIg1gCQALEGgASINQAkQKwBIAFiDQAJEGsASIBYA0ACxBoAEiDWAJAAsQaABIg1ACRArAEgAWINAAkQawBIgFgDQALEGgASINYAkACxBoAEiDUAJECsASABYg0ACRBrAEhgZqcXQG7ffPNN7dnVq1cXXfeSJUuK5l977bXas7aLrrtJL774YtH8gQce2NAmI+bNm1d79uCDD25wE4zGkTUAJECsASCBlp4Gsb1R0nZJuyTtjIjBdiwFAPixdjxn/c8R8UkbrgcAMA6eBgGABFqNdUh60fZbthe3YyEAwF9r9WmQEyJis+2/kbTK9nsR8erogSriiyXp0EMPbfHmAGB6aunIOiI2Vz9vk/SMpIVjzCyPiMGIGBwYGGjl5gBg2pp0rG3va3v/7z6W9AtJ69q1GADgB608DTJb0jPVO8FmSnosIv6rLVsBAH5k0rGOiA8k/UMbdwEAjINzg3SB4eHhovk777yzoU3KffXVV7VnH3jggQY3kWbMyPlK1NNOO63TK/zIcccdV3v2+eefL7ruWbNmla6DSs7f3QAwzRBrAEiAWANAAsQaABIg1gCQALEGgASINQAkQKwBIAFiDQAJEGsASIBYA0ACnBukCxx//PFF8x999FFDmwDSm2++WXt2wYIFRde9ZMmSovlbbrmlaL6XcWQNAAkQawBIgFgDQALEGgASINYAkACxBoAEiDUAJECsASABYg0ACRBrAEiAt5t3gSuuuKJo/sYbb2xok3L9/f21Z++9996i677jjjuK5tesWVM0j9Z9+eWXRfMrV64sml+8eHHt2YMPPrjourPhyBoAEiDWAJAAsQaABIg1ACRArAEgAWINAAkQawBIgFgDQALEGgASINYAkACxBoAEODdIF7j00kuL5i+77LKGNik3Y0b9/98feOCBRdd9xhlnFM0vW7as9uxVV11VdN3nnntu0fz69etrzw4PDxddd2Yffvhh0fwXX3xRe5ZzgwAAOo5YA0ACxBoAEiDWAJAAsQaABIg1ACRArAEgAWINAAkQawBIgFgDQALEGgAS4NwgXaDkPBKSdOKJJza0SXcpPZfI0qVLm1lE0lNPPVU0f9FFF9WeXbVqVek6XWOfffYpmn/uueeK5o888sii+V7GkTUAJDBhrG0/bHub7XWjLjvI9irb71c/9ze7JgBMb3WOrH8v6fSfXHaDpJciYoGkl6rPAQANmTDWEfGqpM9+cvFZkh6pPn5E0tlt3gsAMMpkn7OeHRFbqo8/ljS7TfsAAMbQ8jcYIyIkxXhft73Y9pDtoen0L2IAQDtNNtZbbc+RpOrnbeMNRsTyiBiMiMGBgYFJ3hwATG+TjfUKSYuqjxdJerY96wAAxlLnpXuPS/pfSUfa3mT715Jul/Rz2+9LOrX6HADQkAnfwRgRF47zpVPavAsAYBy83bwLTJe3j+/YsaNo/quvviqaP++884rmS3z66adF82vXrm1ok+YdcMABtWdXrFhRdN0LFy4sXQcV3m4OAAkQawBIgFgDQALEGgASINYAkACxBoAEiDUAJECsASABYg0ACRBrAEiAWANAApwbBC0pOd/HNddcU3Td999/f+k6GEN/f9m/Z/3CCy/Unj3++ONL18EkcWQNAAkQawBIgFgDQALEGgASINYAkACxBoAEiDUAJECsASABYg0ACRBrAEiAWANAApwbBC35+uuva89yro/OeOKJJ4rmOd9Hd+LIGgASINYAkACxBoAEiDUAJECsASABYg0ACRBrAEiAWANAAsQaABIg1gCQAG83R0tmzqz/W+jkk08uuu6XX365dB2M4dJLLy2af/3112vPzp07t3QdTBJH1gCQALEGgASINQAkQKwBIAFiDQAJEGsASIBYA0ACxBoAEiDWAJAAsQaABIg1ACTAuUHQkr333rv27BNPPFF03evXry9dpzGnnnpq0fyOHTsa2qTc5s2bi+Yff/zx2rPXX3996TqYJI6sASCBCWNt+2Hb22yvG3XZUtubba+ufpzZ7JoAML3VObL+vaTTx7h8WUQcU/1Y2d61AACjTRjriHhV0mdTsAsAYBytPGd9pe011dMk/W3bCADwVyYb6/slHS7pGElbJN013qDtxbaHbA8NDw9P8uYAYHqbVKwjYmtE7IqIbyU9KGnhbmaXR8RgRAwODAxMdk8AmNYmFWvbc0Z9eo6kdePNAgBaN+GbYmw/LukkSbNsb5L0W0kn2T5GUkjaKOnyBncEgGlvwlhHxIVjXPxQA7sAAMbBOxgBIAFHxJTd2ODgYAwNDU3Z7QHtsnPnzqL5pUuX1p697bbbCrdpVl9fX+3Z0vO3HH744aXr9LzBwUENDQ15ojmOrAEgAWINAAkQawBIgFgDQALEGgASINYAkACxBoAEiDUAJECsASABYg0ACRBrAEhgwrPuAZBmziz7o7Jo0aLasw888EDRdX/++edF86V27dpVe3Yqzy003XFkDQAJEGsASIBYA0ACxBoAEiDWAJAAsQaABIg1ACRArAEgAWINAAkQawBIgLebAw0YGBioPbvnnns2uEm5m2++ufbsYYcd1uAmGI0jawBIgFgDQALEGgASINYAkACxBoAEiDUAJECsASABYg0ACRBrAEiAWANAAsQaABLg3CBADZ9++mnR/Pnnn197duvWraXrNGr//fevPdvX19fgJhiNI2sASIBYA0ACxBoAEiDWAJAAsQaABIg1ACRArAEgAWINAAkQawBIgFgDQALEGgAS4NwgNb333nu1ZxctWtTgJmUee+yxovn+/v6GNpH22muvovldu3YVzX/zzTe1Z5ctW1Z03U8++WTR/IYNG4rmgYlMeGRte57tV2y/a/sd21dVlx9ke5Xt96ufm/tTDgDTXJ2nQXZKujYijpL0T5J+Y/soSTdIeikiFkh6qfocANCACWMdEVsi4u3q4+2S1ks6RNJZkh6pxh6RdHZTSwLAdFf0DUbb8yUdK+kNSbMjYkv1pY8lzW7rZgCA79WOte39JD0l6eqI+HL01yIiJMU4v26x7SHbQ8PDwy0tCwDTVa1Y295DI6F+NCKeri7eantO9fU5kraN9WsjYnlEDEbE4MDAQDt2BoBpp86rQSzpIUnrI+LuUV9aIem716gtkvRs+9cDAEj1Xmf9M0m/krTW9urqspsk3S7pSdu/lvSRpAuaWREAMGGsI+KPkjzOl09p7zoAgLHwdnMASIC3m9e0ffv22rNDQ0MNblLmiCOO6PQK3zvzzDOL5jdt2lQ0v2bNmqL56eLaa68tmj/lFP7C3I04sgaABIg1ACRArAEgAWINAAkQawBIgFgDQALEGgASINYAkACxBoAEiDUAJECsASABzg2CKbNy5cpOr9CVDjrooKL5vr6+ovnrr7++aJ5/JKQ7cWQNAAkQawBIgFgDQALEGgASINYAkACxBoAEiDUAJECsASABYg0ACRBrAEiAWANAApwbpKY5c+bUnr3uuuuKrvuee+4pmv/666+L5tG6JUuWFM2XnL/j1ltvLbruvfbaq2gevYEjawBIgFgDQALEGgASINYAkACxBoAEiDUAJECsASABYg0ACRBrAEiAWANAArzdvKa5c+fWnr3jjjuKrvvoo48umr/kkkuK5rO67777iuaPO+64hjaRjj322KL5GTM4DkJ78TsKABIg1gCQALEGgASINQAkQKwBIAFiDQAJEGsASIBYA0ACxBoAEiDWAJAAsQaABDg3SBe4+OKLG50HkB9H1gCQwISxtj3P9iu237X9ju2rqsuX2t5se3X148zm1wWA6anO0yA7JV0bEW/b3l/SW7ZXVV9bFhG/a249AIBUI9YRsUXSlurj7bbXSzqk6cUAAD8oes7a9nxJx0p6o7roSttrbD9su7/NuwEAKrVjbXs/SU9JujoivpR0v6TDJR2jkSPvu8b5dYttD9keGh4ebsPKADD91Iq17T00EupHI+JpSYqIrRGxKyK+lfSgpIVj/dqIWB4RgxExODAw0K69AWBaqfNqEEt6SNL6iLh71OVzRo2dI2ld+9cDAEj1Xg3yM0m/krTW9urqspskXWj7GEkhaaOkyxvZEABQ69Ugf5TkMb60sv3rAADGwjsYASABYg0ACRBrAEiAWANAAsQaABIg1gCQALEGgASINQAkQKwBIAFiDQAJEGsASIBYA0ACxBoAEiDWAJAAsQaABIg1ACRArAEgAWINAAkQawBIgFgDQALEGgASINYAkACxBoAEiDUAJOCImLobs4clfTTGl2ZJ+mTKFukc7mfvmS73lfvZnL+NiIGJhqY01uMuYQ9FxGCn92ga97P3TJf7yv3sPJ4GAYAEiDUAJNAtsV7e6QWmCPez90yX+8r97LCueM4aALB73XJkDQDYjY7G2vbptv9ke4PtGzq5S9Nsb7S91vZq20Od3qddbD9se5vtdaMuO8j2KtvvVz/3d3LHdhjnfi61vbl6TFfbPrOTO7aD7Xm2X7H9ru13bF9VXd5Tj+lu7mfXPqYdexrEdp+k/5P0c0mbJL0p6cKIeLcjCzXM9kZJgxHRU69VtX2ipL9I+o+I+PvqsjslfRYRt1f/E+6PiH/t5J6tGud+LpX0l4j4XSd3ayfbcyTNiYi3be8v6S1JZ0v6F/XQY7qb+3mBuvQx7eSR9UJJGyLig4jYIekPks7q4D6YhIh4VdJnP7n4LEmPVB8/opE/BKmNcz97TkRsiYi3q4+3S1ov6RD12GO6m/vZtToZ60Mk/XnU55vU5f+xWhSSXrT9lu3FnV6mYbMjYkv18ceSZndymYZdaXtN9TRJ6qcGfsr2fEnHSnpDPfyY/uR+Sl36mPINxqlzQkT8o6QzJP2m+mt1z4uR59l69SVH90s6XNIxkrZIuquz67SP7f0kPSXp6oj4cvTXeukxHeN+du1j2slYb5Y0b9Tnc6vLelJEbK5+3ibpGY08DdSrtlbPCX733OC2Du/TiIjYGhG7IuJbSQ+qRx5T23toJGCPRsTT1cU995iOdT+7+THtZKzflLTA9mG295T0S0krOrhPY2zvW30TQ7b3lfQLSet2/6tSWyFpUfXxIknPdnCXxnwXr8o56oHH1LYlPSRpfUTcPepLPfWYjnc/u/kx7eibYqqXxfy7pD5JD0fEv3VsmQbZ/juNHE1L0kxJj/XKfbX9uKSTNHK2sq2SfivpPyU9KelQjZxl8YKISP3NuXHu50ka+etySNoo6fJRz+umZPsESf8jaa2kb6uLb9LI87k985ju5n5eqC59THkHIwAkwDcYASABYg0ACRBrAEiAWANAAsQaABIg1gCQALEGgASINQAk8P8yiui6WskSfwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_pixels(px):\n",
    "    fig = plt.figure(figsize = (6,6))\n",
    "    plt.imshow(px, cmap='Greys')\n",
    "\n",
    "show_pixels(unstack(27))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A4.__ _(2 pts)_ Now let's start our analysis by exploring how PCA's ability to reduce dimensionality can be understood as a way to _reduce the resolutiuon of data_. So, start by importing `PCA` from `sklearn.decomposition`, and create three instances: `pca5`, `pca50`, and `pca75`, which will reduce dimensionality to explain only 5%, 50%, and 75% of the dataset's variance, respectively. Then for each, run `.fit(mnist.data)` to train a `PCA` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=0.75, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Make an instance of the Model\n",
    "pca5 = PCA(.05)\n",
    "pca50 = PCA(.5)\n",
    "pca75 = PCA(.75)\n",
    "\n",
    "pca5.fit(mnist.data)\n",
    "pca50.fit(mnist.data)\n",
    "pca75.fit(mnist.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A5.__ _(4 pts)_ Now, using your `pca5`, `pca50`, and `pca75` models, create a function called `deresolve(x, pca)` that accepts an image/record `x` from `mnist.data` and a trained `pca` model to construct an approximation, `deresolved` image. This can be done using `pca.transform()` to construct the dimensionally `reduced` representation of an image as output, which can then be passed to the `pca.inverse_transform()` function to create the `deresolved` pixels, which should be `return`ed by the function.\n",
    "\n",
    "When complete, pass the output of your function's application to your chosen example image to your `show_pixels()` function for each of the `pca5`, `pca50`, and `pca75` models and comment in the response box below how these pca models are abstracting the key features of the data, i.e., shapes of numbers.\n",
    "\n",
    "Note: the `pca.transform()` method expects an array of records as input, so if your chosen example image is `mnist.data[i]`, then you'll have to pass `[mnist.data[i]]` to the `.transform()` function to get it to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def deresolve(x, pca):\n",
    "    reduced = pca.transform([x])\n",
    "    deresolved = pca.inverse_transform(reduced)\n",
    "    return deresolved\n",
    "\n",
    "show_pixels(deresolve(mnist.data[27], pca5).reshape(28,28))\n",
    "show_pixels(deresolve(mnist.data[27], pca50).reshape(28,28))\n",
    "show_pixels(deresolve(mnist.data[27], pca75).reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A6.__ _(3 pts)_ Before we get too deep in with analysis, it would be a good idea to separate our data by training and testing. But this brings up an important question: what are we trying to predict? Well we're working with the `mnist.data` pixels to predict the true number that an individual had hand drawn. These `labels` are available to us through the downloaded content from __A1__ via the attribute `mnist.target`. So, use `train_test_split` from `sklearn.model_selection` to split up the data, and store them as `train`, `test`, `train_labels`, `test_labels`. Be sure to select an appropriate `test_size` and `random_state` for the split, and justify these settings in the response box below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# test_size: what proportion of original data is used for test set\n",
    "train, test, train_labels, test_labels = train_test_split(mnist.data, mnist.target, test_size=1/10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A7.__ _(3 pts)_ Now that we have access to `sklearn` we should use it for all it's worth! We'll do well to standardize our data to move forward with classificaition, but since `sklearn` has the `StandardScaler` function from `sklearn.processing` we can use that! In particular, import `StandardScalar` and `.fit()` a model to the `train` portion of the data set. Then, use the `.transform()` method on the resulting trained standardization object to produce standardized versions of `train` and `test`, calling them `train_std` and `test_std`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "train_std = scaler.transform(train)\n",
    "test_std = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A8.__ _(3 pts)_ Now it's time to apply PCA for the purposes of classification! However, we'll have to re-train a PCA model _on the `train`ing set_ (only) for our desired variance percentage that we'd like to retain. So, write a function called `reduce(percentage)` that:\n",
    "\n",
    "1. creates a `PCA` instance for the desired `percentage` variance explained,\n",
    "2. runs `.fit()` on the standardized `train`ing data `train_std`,\n",
    "3. applies the resulting `.transform()` method to both `train_std` and `test_std`, returning the results.\n",
    "\n",
    "When complete, apply this function using a `percentage` for `95%` of the variance explained and store the outputs as `train_std_pca`, and `test_std_pca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(percentage):\n",
    "    pca = PCA(percentage)\n",
    "    pca.fit(train_std)\n",
    "    return pca.transform(train_std), pca.transform(test_std)\n",
    "\n",
    "train_std_pca, test_std_pca = reduce(.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A9.__ _(7 pts)_ Now it's time to train a classifier. In particular, your job here is to write a function called `train_model(train_dat, train_lbl)` that accepts training data (however pre-processed) with labels and trains. To do this, utilize `LogisticRegression` from `sklearn.linear_model`, but be sure to utilize the following arguments for initializing the model's instance: \n",
    "- `solver = 'lbfgs'`,\n",
    "- `max_iter=1000`, and \n",
    "- `multi_class='ovr'`. \n",
    "\n",
    "Once you've initialized the logistic regression model, apply the `.fit()` method to the data input to your function and return the resulting classifier object. When this function is complete, apply it to the dimensionally reduced `train_std_pca` data output in __A8__ and store the resulting classifier as `clf`.\n",
    "\n",
    "Note: The first two arguments to `LogisticRegression` specify the `solver` being used to optimize the regression (as in __Chapter 7__), and the maximum number of iterations to optimize over. However, the `multi_class='ovr'` argument is new to us! In particular, since we're trying to pick one from 10 discrete labels, i.e., `'0'`, `'1'`, `'2'`, etc., for each input record this is called a _multi-class_ classification problem. Since logistic regression isn't built for this kind of application there has to be a way to modify/enhance the algorithm to support it. As it turns out there's one way in which we can modify any binary classifier into a multi-class classifier, which is what `multi_class='ovr'` does. In particular, `'ovr'` indicates the _one vs. rest_ strategy which trains as many classifiers as there are possible outcomes in the labels. Each of the trained classifiers is trained to positively predict a given label, e.g., `'7'`, and negatively predict against all other, e.g., `not '7'`. All classifiers (10 for us) are then applied to each test record, and the classifier that offers the best positive prediction probability is then corresponds to the predicted label for the record. That's it! While the one vs. rest procedure is _general_ in its ability to convert a binary classifier into a multi-class classifier, it is a limited framework for multi-class classification, and more-precise models are generally those that do best. To see a derived multi-class logistic regression model, check out [Multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression), which can be applied by setting `solver = 'multinomial'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_model(train_dat, train_lbl):\n",
    "    classifier = LogisticRegression(solver = 'lbfgs', max_iter=1000, multi_class='ovr')\n",
    "    classifier.fit(train_std_pca, train_labl)\n",
    "    return classifier\n",
    "\n",
    "clf = train_model(train_std_pca, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A10.__ _(4 pts)_ Now that we have a trained classifier it's time to explore its application to the test set. For this, utilize the `clf.predict()` method on the `test_std_pca` data, storing the result as `test_labels_predicted`. Since this is _not_ a binary classification problem, we likewise can't use binary classification evaluation metrics (i.e., precision, recall, and $F_1$), and so have to fall back on accuracy. So import `accuracy_score` from `sklearn.metrics` and apply it to the pair of `test_labels_predicted` and `test_labels` and report your accuracy. When this is done, go on to compare the first 25 values of `test_labels_predicted` and `test_labels` to see how the model did.  Then, in the response box below discuss\n",
    "1. The types of errors being made by the model, and\n",
    "2. how this output differs from the context of a binary classification problem.\n",
    "\n",
    "Note: for 2, the point to discuss is not over prediction of 0s and 1s, but rather the number of different possible labels that the algorithm is responsible for predicting. This is important, as logistic regression as we've discussed is technically only set up to support binary classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9158571428571428\n",
      "['0' '4' '1' '2' '4' '4' '7' '1' '1' '7' '1' '3' '4' '2' '6' '4' '6' '7'\n",
      " '8' '3' '7' '0' '5' '7' '6']\n",
      "['0' '4' '1' '2' '7' '9' '7' '1' '1' '7' '1' '3' '4' '2' '6' '4' '6' '7'\n",
      " '3' '3' '7' '0' '5' '7' '6']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_labels_predicted = clf.predict(test_std_pca)\n",
    "\n",
    "print(accuracy_score(test_labels_predicted, test_labels))\n",
    "print(test_labels_predicted[:25])\n",
    "print(test_labl[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A11.__ _(8 pts)_ Now that we have our training and testing pipline in place and we've seen what sklearn and logistic regresion can do for image classification, let's explore how `PCA` affects the process. We'll be evaluating how much time each implementation affects the run time, so the first thing you'll want to do is import `datetime`. Following this, write a loop over `portion` values `0.1`, `0.2`, ..., `0.9`. For each `portion`:\n",
    "1. records an initial `datetime.datetime.now()` timestamp as `start`;\n",
    "2. applies your `reduce()` function with the `portion` specified and store the resulting data as `train_std_pca`, `test_std_pca`;\n",
    "3. passes the outputs from 2 to your `train_model()` function and store the classifier as `clf`;\n",
    "4. applies the `clf.predict()` method  to the transformed `test_std_pca` data and store the resulting predictions as `test_labels_predicted`,;\n",
    "5. records a final `datetime.datetime.now()` timestamp as `finish`; and\n",
    "6. `print`s the model `accuracy_score` and execution time (computed as `finish-start`).\n",
    "\n",
    "When this is complete, you should have evaluations for each level of dimensionality reduction provided by PCA. With these in front of you, discuss the effects of PCA on run time and accuracy in the response box below. In particular, at which level of PCA-resolution do you think the best model is produced, considering both accuracy and run time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on classification driven by  0.1  variance explained using PCA...\n",
      "model accuracy:  0.465\n",
      "execution time:  0:00:06.746554\n",
      "\n",
      "Working on classification driven by  0.2  variance explained using PCA...\n",
      "model accuracy:  0.7061428571428572\n",
      "execution time:  0:00:06.893383\n",
      "\n",
      "Working on classification driven by  0.3  variance explained using PCA...\n",
      "model accuracy:  0.7974285714285714\n",
      "execution time:  0:00:06.957588\n",
      "\n",
      "Working on classification driven by  0.4  variance explained using PCA...\n",
      "model accuracy:  0.8558571428571429\n",
      "execution time:  0:00:07.796907\n",
      "\n",
      "Working on classification driven by  0.5  variance explained using PCA...\n",
      "model accuracy:  0.8875714285714286\n",
      "execution time:  0:00:09.806213\n",
      "\n",
      "Working on classification driven by  0.6  variance explained using PCA...\n",
      "model accuracy:  0.9004285714285715\n",
      "execution time:  0:00:16.427101\n",
      "\n",
      "Working on classification driven by  0.7  variance explained using PCA...\n",
      "model accuracy:  0.9072857142857143\n",
      "execution time:  0:00:37.664304\n",
      "\n",
      "Working on classification driven by  0.8  variance explained using PCA...\n",
      "model accuracy:  0.9137142857142857\n",
      "execution time:  0:01:11.281233\n",
      "\n",
      "Working on classification driven by  0.9  variance explained using PCA...\n",
      "model accuracy:  0.9157142857142857\n",
      "execution time:  0:01:42.336083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "for portion in np.array(range(1,10))/10:\n",
    "    print(\"Working on classification driven by \", portion, \" variance explained using PCA...\")\n",
    "    start = dt.now()\n",
    "\n",
    "    train_std_pca, test_std_pca = reduce(portion)\n",
    "    clf = train_model(train_std_pca, train_labels)\n",
    "    test_labels_predicted = clf.predict(test_std_pca)\n",
    "    \n",
    "    finish = dt.now()\n",
    "    print(\"model accuracy: \", accuracy_score(test_labels_predicted, test_labels))\n",
    "    print(\"execution time: \", finish-start)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
