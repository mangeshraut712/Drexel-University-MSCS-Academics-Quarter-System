{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module submission header\n",
    "### Submission preparation instructions \n",
    "_Completion of this header is mandatory, subject to a 2-point deduction to the assignment._ Only add plain text in the designated areas, i.e., replacing the relevant 'NA's. You must fill out all group member Names and Drexel email addresses in the below markdown list, under header __Module submission group__. It is required to fill out descriptive notes pertaining to any tutoring support received in the completion of this submission under the __Additional submission comments__ section at the bottom of the header. If no tutoring support was received, leave NA in place. You may as well list other optional comments pertaining to the submission at bottom. _Any distruption of this header's formatting will make your group liable to the 2-point deduction._\n",
    "\n",
    "### Module submission group\n",
    "- Group member 1\n",
    "    - Name: Mangesh Raut\n",
    "    - Email: mbr63@drexel.edu\n",
    "- Group member 2\n",
    "    - Name: Josh Clark\n",
    "    - Email: jc4577@drexel.edu\n",
    "- Group member 3\n",
    "    - Name: Mobin Rahimi\n",
    "    - Email: mr3596@drexel.edu\n",
    "- Group member 4\n",
    "    - Name: Will Wu\n",
    "    - Email: ww437@drexel.edu\n",
    "\n",
    "### Additional submission comments\n",
    "- Tutoring support received: NA\n",
    "- Other (other): NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment group 2: Network and exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module A _(65 pts)_ Exploring averages, sentiment, and time series\n",
    "In this section we're going to experiment with the word-based text sentiment data generated by a research project documented in the following publication:\n",
    "\n",
    "- https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026752#pone.0026752.s001\n",
    "\n",
    "These data are packaged with the assignment in the following directory:\n",
    "\n",
    "- `./data/Data_Set_S1.txt`\n",
    "\n",
    "Here's the caption describing the data, from their paper:\n",
    "\n",
    "> Data from Mechanical Turk study. labMT 1.0 = language assessment by Mechanical Turk 1.0. In the supplementary tab-delimited file named Data Set S1, we provide our set of 10,222 words, their average happiness evaluations according to users on Mechanical Turk, and other information as described below. Please cite the present paper when using this word set. Within papers, we suggest using the abbreviation labMT 1.0 when referencing this data set. The words are ordered according to average happiness (descending), and the file contains eight columns: (1) word, (2) rank, (3) average happiness (50 user evalutions), (4) standard deviation of happiness, (5) Twitter rank, (6) Google Books rank, (7) New York Times rank, (8) Music Lyrics rank. The last four columns correspond to the ranking of a word by frequency of occurrence in the top 5000 words for the specified corpus. A double dash ‘–’ indicates a word was not found in the most frequent 5000 words for a corpus. Please see the main paper for more information regarding this data set.\n",
    "\n",
    "Note, the paper refers to the scorings as 'happiness' values, but these are also referred to as 'valence', which is a measure of poistive/negative 'affect', or 'sentiment'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A1.__ _(2 pts)_ To start, write a function called `load_labMT`, which takes a string argument called `path_to_labMT` that contains the path to a `.txt` file containing the data and loads the dataset with `pandas` into a dataframe.\n",
    "\n",
    "\\[Hint. Since the folks who put these data together put a few lines (3) of descriptive text&mdash;not data&mdash;use the `skiprows` argument in `pd.read_csv()` to start parsing/loading into the dataframe at the appropriate line. Just be sure to keep the header!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1:Function(2/2)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_labMT(path_to_labMT):\n",
    "\n",
    "    #--- Your code starts here\n",
    "\n",
    "    #--- Your code ends here\n",
    "\n",
    "    return labMT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your function, print its `.head()`. Your output should look like this:\n",
    "\n",
    "```\n",
    "        word  happiness_rank  happiness_average  happiness_standard_deviation  \\\n",
    "0   laughter               1               8.50                        0.9313   \n",
    "1  happiness               2               8.44                        0.9723   \n",
    "2       love               3               8.42                        1.1082   \n",
    "3      happy               4               8.30                        0.9949   \n",
    "4    laughed               5               8.26                        1.1572   \n",
    "\n",
    "  twitter_rank google_rank nyt_rank lyrics_rank  \n",
    "0         3600          --       --        1728  \n",
    "1         1853        2458       --        1230  \n",
    "2           25         317      328          23  \n",
    "3           65        1372     1313         375  \n",
    "4         3334        3542       --        2332  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1:SanityCheck\n",
    "\n",
    "labMT = load_labMT(\"./data/Data_Set_S1.txt\")\n",
    "\n",
    "print(labMT.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A2.__ _(3 pts)_ Write a function called  `make_histogram` which plots a 25-bin histogram of the `'happiness_average'` column. In this, compute and plot the locations of the `mean` and `median` as `'red'` and `'blue'` vertical lines, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2:Function(2/3)\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def make_histogram(labMT):\n",
    "    \n",
    "    fig = plt.figure(figsize = (6,6))\n",
    "    \n",
    "    #--- Your code starts here\n",
    "\n",
    "    \n",
    "    #--- Your code starts here\n",
    "    \n",
    "    axes = plt.gca()\n",
    "    plt.plot([mean, mean], axes.get_ylim(), color = 'red')\n",
    "    plt.plot([median, median], axes.get_ylim(), color = 'blue')\n",
    "    plt.ylabel('Frequency', fontsize = 15)\n",
    "    plt.xlabel('Valence', fontsize = 15)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test your function, the resulting figure should look like this:\n",
    "\n",
    "![Expected Output](img/a2-expected-output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2:SanityCheck\n",
    "\n",
    "fig = make_histogram(labMT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2:Inline(1/3)\n",
    "\n",
    "# Look at the visualization, is the median or mean larger? Print \"Median\" or \"Mean\".\n",
    "print(\"<Your Answer:Median or Mean>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A3.__ _(3 pts)_ Now complete the `plot_average_vs_deviation` function, which compares the `'happiness_average'` (created as an average of 50 survey responses) `'happiness_standard_deviation'` column from these 50-respondant cohorts. In particular, make a scatter plot of the `'happiness_standard_deviation'` (vertical) against the `'happiness_average'` (horizontal) column. Again, be sure to clearly label axes and adjust any arguments to make this picture as clear and interpretable as possible.\n",
    "\n",
    "\\[Hint. Be sure to utilize the `color` and `alpha` arguments to highlight variations in point density.\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A3:Function(2/3)\n",
    "\n",
    "def plot_average_vs_deviation(labMT):\n",
    "    fig = plt.figure(figsize = (6,6))\n",
    "    \n",
    "    #--- Your code starts here\n",
    "\n",
    "    \n",
    "    #--- Your code ends here\n",
    "    \n",
    "    plt.ylabel('Valence $\\sigma$', fontsize = 15)\n",
    "    plt.xlabel('Valence', fontsize = 15)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test your function, the resulting figure should look like this:\n",
    "\n",
    "![Expected Output](img/a3-expected-output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A3:SanityCheck\n",
    "\n",
    "fig = plot_average_vs_deviation(labMT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A3:Inline(1/3)\n",
    "\n",
    "# Look at the visualization, is the distribution denser at left or right? Print \"Left\" or \"Right\".\n",
    "print(\"<Your Answer:Left or Right>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A4.__ _(2 pts)_ One of the columns is labeled `'nyt_rank'`, which indicates the rank (by frequency) of each given word in a corpus of all New York Times (NYT) articles from the 20-year period, 1987&ndash;2007. This is provided because the ~10k words that constitute the data were, in part, drawn from this 20-year corpus of articles. Thus, the words in `./data/Data_Set_S1.txt` data are specially tuned for application to a corpus text drawn from the NYT.\n",
    "\n",
    "So to match, frequencies are provided for the same ~10k words for each daily issue of the NYT in the 20-year corpus. These are contained in the file:\n",
    "\n",
    "- `\"./data/nyt.csv\"`\n",
    "\n",
    "which will available in the data directory after the compressed file is inflated, i.e., after unzipping from the command line in the data directory:\n",
    "\n",
    "```\n",
    ">>> unzip nyt.csv.zip\n",
    "```\n",
    "\n",
    "Load these data using `pandas`, store the `.transpose()`'d dataframe as `nyt`, and then set the dataframe's `.columns` attribute to the `'word'` column of the valence dataframe. \n",
    "\n",
    "Note: with these data, we're thinking about _time_ as the independent, '`x`' variable, so it's convenient to have the times along the rows, and words along the columns&mdash;this is the 'why' for the application of `.transpose()` from the shape of the `nyt` data on file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._ There were $7475$ times and $10222$ words, making the dimension compatible with the valence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A4:Function(2/2)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_nyt(path_to_nyt, labMT):\n",
    "\n",
    "    #--- Your code starts here\n",
    "\n",
    "    \n",
    "    #--- Your code ends here\n",
    "\n",
    "    return nyt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, print the `.shape` of the `nyt` data to confirm they are compatible with the valence data, e.g., for inner products. Your output should look like:\n",
    "```\n",
    "(7475, 10222)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A4:SanityCheck\n",
    "\n",
    "# load nyt and print its shape (matrix dimensions)\n",
    "nyt = load_nyt('./data/nyt.csv', labMT)\n",
    "print(nyt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A5.__ _(5 pts, total)_ While the valence values were detemined by survey and may be skewed, the histogram from __A2__ doesn't reflect the frequency with which the words were actually used. For this part, your job is to repeate the histogram picture, but with weighted values, including the computation of mean and median lines.\n",
    "\n",
    "So, write a function that computes the mean and median as weighted quantities, according to the occurrence of words in the articles. So intuitively, if the most positive word (`'laughter'`, `happiness_average = 8.5`) appeared 100 times across the NYT, there would be 100 instances of the value 8.5 to incorporate into each of the mean and median calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Weights.__ To start, total the frequencies in the NYT data across time (collapsing rows) into a single vector: $\\vec{F} = [F_1, \\cdots, F_n]$ (use the `axis` argument in the `.sum()` method and name the resulting object `F`). Using this, record the total number of words in the entire dataset: `N = F.sum()`. Print `N` once calculated, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A5:Inline(1/5)\n",
    "\n",
    "# total the frequencies in the NYT data across time (collapsing rows) into a single vector\n",
    "# record the total number of words in the entire dataset (sum F)\n",
    "\n",
    "#--- Your code starts here\n",
    "\n",
    "    \n",
    "#--- Your code ends here\n",
    "\n",
    "# print the number of words in the entire dataset, it should be 868729965\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mean.__ For the mean you'll have to use $\\vec{F}$ in a sum formula that groups the contributions of words according to their $F_i$ weights:\n",
    "$$\n",
    "\\overline{h} = \\frac{\\sum_{i=1}^nh_i\\cdot F_i}{\\sum_{i=1}^nF_i} = \\frac{1}{N}\\sum_{i=1}^nh_i\\cdot F_i,\n",
    "$$\n",
    "Here, the $h_i$ indicate the valence values for the words, and the denominator $N = \\sum_{i=1}^nF_i$ is equal to the total number of words that appeared in the dataset (as above). Print the mean once calculated, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A5:Inline(1/5)\n",
    "\n",
    "# average the `'happiness_average'` column by using the F weights\n",
    "\n",
    "#--- Your code starts here\n",
    "\n",
    "    \n",
    "#--- Your code ends here\n",
    "\n",
    "# print the average of average hapiness, it should be about 5.3687\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Median.__ While the above mean can be easily computed  via sums and dot products the 'weighted' median is more challenging. In particular, the median considers the middle-most word according to a sort from low to high by valence. \n",
    "\n",
    "Since we now have frequencies of occurrence for each word, we have to _cumulatively_ count up the frequencies in order of increasing (or decreasing) valence. The median will be the word/valence over which half&mdash;$N/2$&mdash;of the frequency in $\\vec{F}$ accumulates.\n",
    "\n",
    "The valence data are already sorted from high to low. But this is a special case&mdash;we're looking for the 50th percentile. So, we can take cumulative sums of `F` to identify the word for which $N/2$ accumulates. In particular, use the `.cumsum()` method on `F` to identify the median word/valence appearing at $N/2$. Print the median once calculated, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A5:Inline(1/5)\n",
    "\n",
    "# find the median of the weighted distribution using a cumulative sum of F\n",
    "\n",
    "#--- Your code starts here\n",
    "\n",
    "    \n",
    "#--- Your code ends here\n",
    "\n",
    "# print the weighted median of average happiness, it should be about 5.24\n",
    "print(median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Histogram.__ Now write an updated function, called `make_weighted_histogram` which uses $\\vec{F}$ as a `weights` argument for the histogram to depict your weighted `mean` and `median` from the above parts of this section. Note: you may reuse much of your histogram code from __A2__ for the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A5:Function(1/5)\n",
    "\n",
    "def make_weighted_histogram(labMT, nyt):\n",
    "    \n",
    "    fig = plt.figure(figsize = (6,6))\n",
    "    \n",
    "    #--- Your code starts here\n",
    "    \n",
    "    \n",
    "    #--- Your code starts here\n",
    "    \n",
    "    axes = plt.gca()\n",
    "    plt.plot([mean, mean], axes.get_ylim(), color = 'red')\n",
    "    plt.plot([median, median], axes.get_ylim(), color = 'blue')\n",
    "    plt.ylabel('Frequency', fontsize = 15)\n",
    "    plt.xlabel('Valence', fontsize = 15)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test your function, the resulting figure should look like this:\n",
    "\n",
    "![Expected Output](img/a5-expected-output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A5:SanityCheck\n",
    "\n",
    "fig = make_weighted_histogram(labMT, nyt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A5:Inline(1/3)\n",
    "\n",
    "# Look at the weighted visualization, is the median or mean larger? Print \"Median\" or \"Mean\".\n",
    "print(\"<Your Answer:Median or Mean>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A6.__ _(2 pts)_ The NYT data are timeseries data, but how can we utilize the temporal information? To get started, modify `nyt`'s to contain parsed `datetime` objects, utilizing the `dateutil.parser.parse()` function. \n",
    "\n",
    "\\[Hint: to get/modify the index in a dataframe utilize the `.index` attribute.\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A6:Inline(2/2)\n",
    "\n",
    "#--- Your code starts here\n",
    "\n",
    "    \n",
    "#--- Your code ends here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this is done, print the first 10 elements of the index. Your output should look like this:\n",
    "\n",
    "```\n",
    "DatetimeIndex(['1987-01-01', '1987-01-02', '1987-01-03', '1987-01-04',\n",
    "               '1987-01-05', '1987-01-06', '1987-01-07', '1987-01-08',\n",
    "               '1987-01-09', '1987-01-10'],\n",
    "              dtype='datetime64[ns]', freq=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A6:SanityCheck\n",
    "\n",
    "nyt.index[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A7.__ _(3 pts)_ Throughout this assignment we're going to want to be able to standardize the columns of our time series. So, your job here is to write a function called `standardize(ts)` that accepts a time series dataframe `ts` (like `nyt`) as input, and outputs a timeseries of same shape, containing the standardized columns of `ts`.\n",
    "\n",
    "For `nyt`, standardizing the columns will allow comparability words, since they occur at very different frequencies. So when `standardize()` processes `nyt`, the $i^\\text{th}$ word at time $t$ should be transformed to: \n",
    "$$\n",
    "\\frac{f_{t,i} - \\mu_T(f_i)}{\\sigma_T(f_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A7:Function(2/3)\n",
    "\n",
    "def standardize(ts):\n",
    "    \n",
    "    #--- Your code starts here\n",
    "    \n",
    "\n",
    "    \n",
    "    #--- Your code ends here\n",
    "    \n",
    "    return std_ts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this is complete, store the application of `standarize()` to `nyt` as `word_series` and exhibit its `.head(10)`. Your output should be:\n",
    "\n",
    "```\n",
    "1987-01-01    4.413570\n",
    "1987-01-02    4.694376\n",
    "1987-01-03    1.699111\n",
    "1987-01-04    4.507172\n",
    "1987-01-05    1.605509\n",
    "1987-01-06    1.792714\n",
    "1987-01-07    0.201479\n",
    "1987-01-08   -0.172929\n",
    "1987-01-09    0.482285\n",
    "1987-01-10   -0.360133\n",
    "Name: year's, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A7:SanityCheck\n",
    "word_series = standardize(nyt)\n",
    "print(word_series[\"year's\"].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A7:Inline(1/3)\n",
    "\n",
    "# Look at your output, does the word occur at above- or \n",
    "# below-average frequency at the beginning of the year? \n",
    "# Print \"Above\" or \"Below\".\n",
    "print(\"<Your Answer:Above or Below>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A8.__ _(10 pts)_ Now write a function called `plot_series(ts, start, stop)` that plots the columns of a time series `ts`, like `word_series` (or `nyt`), as line plots of a range in time specified by strings `start` and `stop` in the format `yyyy-mm-dd`. Be sure to make your visualization as interpretable as possible, specifically utilizing both `label`s for columns and transparency of line color (`alpha`).\n",
    "\n",
    "\\[Hint. to retrieve values in the temporal plotting range defined by `start` and `stop`, use boolean masks.\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A8:Function(8/10)\n",
    "\n",
    "def plot_series(ts, start, stop, ylabel, legend = True):\n",
    "    \n",
    "    #--- Your code starts here\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #--- Your code ends here\n",
    "    \n",
    "    plt.ylabel(ylabel, fontsize = 15)\n",
    "    plt.xlabel(\"Date\", fontsize = 15)\n",
    "    if legend:\n",
    "        plt.legend()\n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For development and testing, select just a few word-columns: `word_series[words]` (slicing out columns results in a smaller set of valid series) and a well-understood range of time (`start`/`stop`). In particular, use:\n",
    "\n",
    "- `words = ['new'`, `'year'`, `\"year's\"`, `'two'`, `'eve'`, `'thousand']`\n",
    "- `start = '1999-12-15'`\n",
    "- `stop = '2000-01-15'` \n",
    "\n",
    "for development, and review the output of your function in the sanity check, below. It should look like:\n",
    "\n",
    "![Expected Output](img/a8-expected-output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A8:SanityCheck\n",
    "\n",
    "words = ['new', 'year', \"year's\", 'thousand', 'two', 'eve']        \n",
    "fig = plot_series(word_series[words], \"12-15-1999\", \"01-15-2000\", \"Standardized word usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A8:Inline(2/10)\n",
    "\n",
    "# Look closely at your output, what word seems to associate with the\n",
    "# standardized usage of \"year's\" most closely?\n",
    "# Print one of 'new', 'year', 'thousand', 'two', or 'eve'.\n",
    "print(\"<Your Answer: new, year, thousand, two, or eve>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A9.__ _(7 pts)_ One common aspect of time series' is the presence of 'uninteresting' trends (at least from the point of view of the desired analysis). Generally, de-trending can be done by characterizing a trend&mdash;a regularity desired for removal&mdash;and  removing it, either by subtraction or division. Right now, our interest will be focused on spikes that may represent events outside of an 'everyday routine', so we will be _de-trending weekly signatures_. \n",
    "\n",
    "So, create a function called `detrend_weekly(ts)` that takes a time series `ts`, computes the average day-of-week values for each column, and the divides column values them by their respective averages from the corresponding days of week. The function should `return` a dataframe the same shape as `ts`.\n",
    "\n",
    "Finally, comment in the markdown cell below as to any changes that occur when the weekly trend is removed. What do you think these changes tell you about the data, i.e., word frequencies in NYT articles? Do you think the issues for the different days of the week were generally of the same size?\n",
    "\n",
    "\\[Hint: Python support 'triple-index' slicing, for which the third index indicates a step-size for the slice, i.e, for a list `lst`, `lst[0:401:10]` will result in every `10`th value from index `0` to `400`, i.e., `[0, 10, 20, ..., 400]`.\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A9:Function(5/7)\n",
    "\n",
    "def detrend_weekly(ts):\n",
    "    \n",
    "    #--- Your code starts here\n",
    "    \n",
    "\n",
    "\n",
    "    #--- Your code ends here\n",
    "    \n",
    "    return(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this is complete, again, apply your code to just the same few word-columns: `word_series[words]`. In partcular, use:\n",
    "\n",
    "- `words = ['new'`, `'year'`, `\"year's\"`, `'two'`, `'eve'`, `'thousand']`\n",
    "\n",
    "and pass the result, `detrend_weekly(word_series[words])` through `plot_series()` over the range of time defined by:\n",
    "\n",
    "- `start = '1999-12-15'`\n",
    "- `stop = '2000-01-15'`\n",
    "\n",
    "Its output should look like:\n",
    "\n",
    "![Expected Output](img/a9-expected-output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A9:SanityCheck\n",
    "\n",
    "fig = plot_series(detrend_weekly(word_series[words]), \"12-15-1999\", \"01-15-2000\", \"Standardized word usage, detrended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A9:Inline(2/7)\n",
    "\n",
    "# Look closely at your output, does removing\n",
    "# the day-of-week signature bring the words 'year' and \"year's\"\n",
    "# closer together or farther apart over this range?\n",
    "print(\"<Your Answer: closer or farther>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A10.__ _(8 pts)_ Next up, your job is to produce a function called `avgs(ts, stop = 0)`, which produces a timeseries of average valence values for a times series, `ts`, e.g., the entire 20-year range of `nyt`. \n",
    "\n",
    "In particular, for a given day, $t$, the average daily value should be\n",
    "\n",
    "$$\n",
    "\\overline{h}_t = \\frac{ \\sum_{i=1}^nh_i\\cdot f_{t, i}}{\\sum_{i=1}^nf_{t,i}},\n",
    "$$\n",
    "\n",
    "The argument `stop` indicates the size of a 'stop word window' (see the valence-data authors' paper to supplement details) of valence values for words to be omitted from the calculation. So, if `stop = 1` and the $i^\\text{th}$ word has valence $h_i$ inside the window (i.e., $h_i\\in [4,6]$), then $h_i\\cdot f_{t, i}$ and $f_{t, i}$ should be excluded from the above sum's numerator and denominator, respectively. \n",
    "\n",
    "\\[Hint: use `.dot()` products and `.sum()` methods on boolean masks of `ts`, slicing the non-stop words for fast calculation.] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A10:Function(6/8)\n",
    "\n",
    "def avgs(ts, stop = 0):\n",
    "    \n",
    "    #--- Your code starts here\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #--- Your code stops here\n",
    "    \n",
    "    return mean_ts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this is complete, store the output of this function applied to `nyt` `for` each value of `stop` in `range(4)` as a column in a new time series dataframe called `avgs_series` under the column names `'avg-'+str(stop)`. Then, display these new columns using your `plot_series()` function over a new range, defined by: \n",
    "\n",
    "- `start = \"2000-01-01\"`\n",
    "- `stop = \"2003-12-31\"`\n",
    "\n",
    "Your output should look like:\n",
    "\n",
    "![Expected Output](img/a10-expected-output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A10:SanityCheck\n",
    "\n",
    "avgs_series = pd.DataFrame({'avg-' + str(stop): avgs(nyt, stop = stop) \n",
    "                            for stop in range(4)}, \n",
    "                           index = nyt.index)\n",
    "    \n",
    "fig = plot_series(avgs_series, \"2000-01-01\", \"2003-12-31\", \n",
    "                  \"Average daily valence, various stop word windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A10:Inline(2/8)\n",
    "\n",
    "# Look closely at your output, as the stop-word removal window widens,\n",
    "# do the magnitude of the values swing wider or narrower?\n",
    "print(\"<Your Answer: wider or narrower>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A11.__ _(8 pts)_ One thing you might've noticed from the previous part was that the calculated averages were rather noisy, preventing us from observing long-term trends and big events. So, let's build a function called `smooth(ts, r = 1)` that takes a time series `ts` (e.g., `avgs_series`) and returns a same-shaped time series dataframe of _moving averages_ of the columns of your `series`.  \n",
    "\n",
    "Averages are to be taken over the values in a 'window' of radius $r$ about a point in time $t$. In particular, the smoothed value at $t_\\text{smooth}$ should be taken over all times $t$ for which \n",
    "\n",
    "$$t_\\text{smooth} - r < t < t_\\text{smooth} + r$$ \n",
    "\n",
    "For example, if we call the result `smooth_ts = smooth(ts, r)`, then the values should be:\n",
    "\n",
    "```\n",
    "smooth_ts.iloc[i,j] = np.mean(ts[i-r:(i+r)+1, j])\n",
    "``` \n",
    "\n",
    "__Important__: for the edge cases, i.e., when `i < r` or when  `i + r > series.shape[0]`, you'll have to take exta care on indexing to make sure your averages are taken over fewer values. This is the hardest component of this part of the assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A11:Function(6/8)\n",
    "\n",
    "def smooth(series, r = 1):\n",
    "    \n",
    "    columns = series.columns\n",
    "    index = series.index\n",
    "    ts = series.to_numpy()\n",
    "    \n",
    "    if r:\n",
    "        #--- Your code starts here\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "        #--- Your code stops here\n",
    "    else:\n",
    "        return series\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When complete, apply `smooth()` to `avgs_series`, using several values of `r`, and describe how the smoothing affects it in the response box below. To exhibit your code's function, set `r=10` and then utilize the corresponding output in `plot_series()`, again over the range defined by: \n",
    "\n",
    "- `start = \"2000-01-01\"`\n",
    "- `stop = \"2003-12-31\"`\n",
    "\n",
    "Your output should look like:\n",
    "\n",
    "![Expected Output](img/a11-expected-output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A11:SanityCheck\n",
    "\n",
    "fig = plot_series(smooth(avgs_series, r = 10), \"2000-01-01\", \"2003-12-31\", \"Smoothed average daily valence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A11:Inline(2/8)\n",
    "\n",
    "# Looking at the smoothed shapes, in what year does the 'least happy' \n",
    "# dip in the time series occur, 2001 or 2003?\n",
    "print(\"<Your Answer: 2001 or 2003>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A12.__ _(2 pts)_ Now, use `plot_series()` again on the `avgs_series` values, but transform them in the following order:\n",
    "\n",
    "1. `detrend_weekly()`\n",
    "- `standardize()`\n",
    "- `smooth()`\n",
    "\n",
    "and be sure to use the value `r=10` from the previous part of this assignment.\n",
    "\n",
    "Then, take the resulting time series and apply `plot_series()` to it over the range defined by: \n",
    "\n",
    "- `start = \"2000-01-01\"`\n",
    "- `stop = \"2003-12-31\"`\n",
    "\n",
    "For reference, your output should look like:\n",
    "\n",
    "![Expected Output](img/a12-expected-output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A12:Inline(1/2)\n",
    "fig = plot_series(smooth(standardize(detrend_weekly(avgs_series)), 10), \n",
    "                  \"2000-01-01\", \"2003-12-31\", \"Smoothed, standardized, average daily valence, detrended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A12:Inline(1/2)\n",
    "\n",
    "# Decide the range of stop-word windows that think produces the most 'stable' shape. \n",
    "# In particular, which two consecutive window sizes appear to exhibit to most similar \n",
    "# variation to each other as timeseries'? Respond with one of: \n",
    "# \"avg-0 ~ avg-1\"; \"avg-1 ~ avg-2\"; \"avg-2 ~ avg-3\"; or \"avg-3 ~ avg-4\"\n",
    "\n",
    "print(\"<Your Answer: avg-0 ~ avg-1; avg-1 ~ avg-2; avg-2 ~ avg-3; or avg-3 ~ avg-4>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A13.__ _(5 pts)_ While smoothing helped us to visually observe some candidate big events on the time series, it will actually make it more difficult to pin down exactly where these shifts happended, and hence provide clues as to _why_ they happened. This is because one means we have for assessing a big 'drop', or, more generally a 'shift', is by looking at outlier _differences_ in the time series values. \n",
    "\n",
    "So, write a new function called `diff(ts)` that calculates _single-step, backward differences_ of `ts` and outputs them as another timeseries dataframe. In particular, this function should subtract the $t-1^\\text{st}$ series values from the $t^\\text{th}$. \n",
    "\n",
    "__Important__: the output of `diff()` should always be one row smaller than its input, since the first (`0`-index) row has none previous to subtract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A13:Function(3/5)\n",
    "\n",
    "def diff(ts):\n",
    "    \n",
    "    #--- Your code starts here\n",
    "    \n",
    "\n",
    "    \n",
    "    #--- Your code starts here\n",
    "    \n",
    "    return diffs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, apply your code in the following order to `stable_series =  avgs_series[[\"avg-1\"]]`:\n",
    "\n",
    "1. `detrend_weekly()`\n",
    "2. `standardize()`\n",
    "3. `diff()`\n",
    "\n",
    "Then, using `.sort(by = best_column)` store the result it as `diffs`, print the `.head()` and `.tail()` of the result. For reference, your output should look like:\n",
    "\n",
    "\n",
    "```\n",
    "               avg-1\n",
    "1999-06-19 -5.829010\n",
    "2001-09-12 -4.232600\n",
    "1988-07-31 -3.646154\n",
    "2003-04-05 -3.560870\n",
    "1998-08-22 -3.480828\n",
    "               avg-1\n",
    "1989-01-21  3.605293\n",
    "1992-07-04  3.893948\n",
    "1987-10-24  4.071007\n",
    "2006-03-29  4.643185\n",
    "1999-06-20  5.286476\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A13:SanityCheck\n",
    "\n",
    "stable_series = avgs_series[[\"avg-1\"]]\n",
    "diffs = diff(standardize(detrend_weekly(stable_series))).sort_values(by = \"avg-1\")\n",
    "print(diffs.head())\n",
    "print(diffs.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the change of sign (negative for `.head()` and positive for `.tail()`), do you recognize any dates that might correspond to sentiment-charged news reporting major geopolitical events?  \n",
    "\n",
    "There are a number of reasonable guesses for the timing of events, of course some are more difficult to find than others:\n",
    "\n",
    "- 1988-07-31: _Negative_, Sultan Abdul Halim ferry terminal bridge collapse\n",
    "    - https://en.wikipedia.org/wiki/Sultan_Abdul_Halim_ferry_terminal_bridge_collapse\n",
    "- 1989-01-20: _Positive_, G. Bush Sr. inaugurated as president of USA\n",
    "    - https://en.wikipedia.org/wiki/Inauguration_of_George_H._W._Bush\n",
    "- 1992-07-04: _Positive_, 4th of July celebration\n",
    "- 1999-06-19: _Negative_, House rejects gun control bill, in year of Columbine shooting\n",
    "    - https://www.nytimes.com/1999/06/19/us/guns-schools-overview-gun-control-bill-rejected-house-bipartisan-vote.html\n",
    "- 2001-09-12: _Negative_, Sept. 11th attacks\n",
    "    - https://en.wikipedia.org/wiki/September_11_attacks\n",
    "- 2003-04-05: _Negative_, Battle of Baghdad\n",
    "    - https://en.wikipedia.org/wiki/Battle_of_Baghdad_(2003)\n",
    "\n",
    "Having google and the web helps with dates, and particularly Wikipedia's event category pages: https://en.wikipedia.org/wiki/Category:July_1988_events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A13:Inline(1/2)\n",
    "\n",
    "# While this does pick out some key dates, this is definitely \n",
    "# not a perferct event detection tool. Sometimes, if an event measured in this way\n",
    "# leaves the 'collective attention' quickly, the diff() tool will \n",
    "# pick up both positive and negative directions, side-by-side. \n",
    "# Determine which of the events exhibits this effect:\n",
    "\n",
    "# 'Bridge Collapse'; 'Inauguration'; 'July 4th'; 'Gun Control'; 'Sept. 11th'; or 'Battle of Baghdad'\n",
    "print(\"<Your Answer: Bridge Collapse; Inauguration; July 4th; Gun Control; Sept. 11th; or Battle of Baghdad>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A14.__ _(10 pts)_ To observe candidate events, modify your original `plot_series` function into a new one:\n",
    "\n",
    "- `plot_series_top_diffs(ts, start, stop, ylabel, top = 5, legend = True)` \n",
    "\n",
    "so as to additionally visualize the `top` (most extreme) values determined from an application of `diff()` to `ts`, i.e., the `.head(top)` and `.tail(top)` from the sorted output of `diff()`.  \n",
    "\n",
    "Beyond `plot_series()`, this function should, intuitively, exhibit 'points on a line' over a specified `start`/`stop` range. As in part __C12__, your final product should exhibit the series with transformations in the following order:\n",
    "\n",
    "1. `detrend_weekly()`\n",
    "- `standardize()`\n",
    "- `smooth()`\n",
    "\n",
    "Again, be sure to use the value `r=10` from the previous part of this assignment.\n",
    "\n",
    "__Important:__ `diff()` should only be applied to _un-smoothed_ data. In other words, for the `2*top` points we'd like to retrieve from `diff` the order of transformations should be:\n",
    "\n",
    "1. `detrend_weekly()`\n",
    "- `standardize()`\n",
    "- `diff()`\n",
    "\n",
    "So, be sure to pass only _detrended_, _standardized_ series to the `plot_series_top_diffs` function. Application of `diff()` and `smooth()` should then be applied to the passed `ts` to identify the `top` times along the `smooth()`'d values for plotting.\n",
    "\n",
    "Also, as with parts __C10__&mdash;__C13__, your final code should be exhibited over the range\n",
    "\n",
    "- `start = \"2000-01-01\"`\n",
    "- `stop = \"2003-12-31\"`\n",
    "\n",
    "but now just in application to the `best_series` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A14:Function(8/10)\n",
    "\n",
    "def plot_series_top_diffs(ts, start, stop, ylabel, top = 5, legend = True):\n",
    "    \n",
    "    #--- Your code starts here\n",
    "    \n",
    "    \n",
    "    #--- Your code stops here\n",
    "    \n",
    "    plt.ylabel(ylabel, fontsize = 15)\n",
    "    plt.xlabel(\"Date\", fontsize = 15)\n",
    "    if legend:\n",
    "        plt.legend()\n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this is complete, exhibit the function's output setting `top = 5`. Then, discuss the locations of these points below in the context of the potential events you identified in __C13__. Is the analysis all consistent?\n",
    "\n",
    "For reference, your output should look like this:\n",
    "\n",
    "![Expected Output](img/a14-expected-output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A14:SanityCheck\n",
    "\n",
    "fig = plot_series_top_diffs(standardize(detrend_weekly(stable_series)),  \n",
    "                            \"2000-01-01\", \"2003-12-31\", \n",
    "                            (\"Average daily valence, using stop window = $5\\pm 1$\\n\" + \n",
    "                             \"(smoothed, standardized, and detrended),\\n\" + \n",
    "                             \"with top single-step backward diffs (points).\"), \n",
    "                            top = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A14:Inline(2/10)\n",
    "\n",
    "# Review the output, what were the top two events \n",
    "# that the timeseries outliers identified?\n",
    "\n",
    "# 'Bridge Collapse'; 'Inauguration'; 'July 4th'; 'Gun Control'; 'Sept. 11th'; or 'Battle of Baghdad'\n",
    "print(\"<Your Two Answers: Bridge Collapse; Inauguration; July 4th; Gun Control; Sept. 11th; or Battle of Baghdad>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
