{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wCcmhNN8WRp"
   },
   "source": [
    "## Module submission header\n",
    "### Submission preparation instructions \n",
    "_Completion of this header is mandatory, subject to a 2-point deduction to the assignment._ Only add plain text in the designated areas, i.e., replacing the relevant 'NA's. You must fill out all group member Names and Drexel email addresses in the below markdown list, under header __Module submission group__. It is required to fill out descriptive notes pertaining to any tutoring support received in the completion of this submission under the __Additional submission comments__ section at the bottom of the header. If no tutoring support was received, leave NA in place. You may as well list other optional comments pertaining to the submission at bottom. _Any distruption of this header's formatting will make your group liable to the 2-point deduction._\n",
    "\n",
    "### Module submission group\n",
    "- Group member 1\n",
    "    - Name: Mangesh Raut\n",
    "    - Email: mbr63@drexel.edu\n",
    "- Group member 2\n",
    "    - Name: Josh Clark\n",
    "    - Email: jc4577@drexel.edu\n",
    "- Group member 3\n",
    "    - Name: Mobin Rahimi\n",
    "    - Email: mr3596@drexel.edu\n",
    "- Group member 4\n",
    "    - Name: Will Wu\n",
    "    - Email: ww437@drexel.edu\n",
    "\n",
    "### Additional submission comments\n",
    "- Tutoring support received: NA\n",
    "- Other (other): NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GP5M9sL88WRt"
   },
   "source": [
    "# Assignment group 1: Textual feature extraction and numerical comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlUk1ANa8WRu"
   },
   "source": [
    "## Module C _(35 points)_ Similarity of word usage across a document\n",
    "\n",
    "Here we'll be building up some code to discover how different terms are utilized similarly across a document. For this, our first task will be to create a word frequency counting function.\n",
    "\n",
    "__C1.__ _(12 points)_ Define a function called `count_words(paragraph, pos = True, lemma = True)` that `return`s a `Counter()` called `frequency`. In `frequency`, each key will consist of a `heading = (text, tag)`, where `text` contains the `word.text` attribute from `spacy` if `lemma = False`, and `word.lemma_` attribute if `True`. Similarly, `tag` should be left empty as `\"\"` if `pos = False` and otherwise contain `word.pos_`. The `Counter()` should simply contain the number of times each `heading` is observed in the `paragraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PLsyDXDa8WRu"
   },
   "outputs": [],
   "source": [
    "# C1:Function(12/12)\n",
    "\n",
    "from collections import Counter\n",
    "import spacy, json, re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def count_words(paragraph, pos = True, lemma = True):\n",
    "\n",
    "    #---Your code starts here\n",
    "    frequency = Counter()\n",
    "    words = nlp(paragraph)\n",
    "\n",
    "    for word in words:\n",
    "        text = word.text\n",
    "        if lemma:\n",
    "          text = word.lemma_\n",
    "        \n",
    "        tag = \"\"\n",
    "        if pos:\n",
    "            tag = word.pos_\n",
    "\n",
    "        # Ignore whitespace and punctuation and only count real words\n",
    "        if tag==\"SPACE\" or tag==\"PUNCT\":\n",
    "          continue\n",
    "        # Remove leading dashes\n",
    "        text = re.sub(\"--(?=\\w+)\", \"\", text)\n",
    "        # Ignore punctuation misidentified as words\n",
    "        if re.fullmatch(\"[?!\\-.*']+\", text):\n",
    "          continue\n",
    "        # Note that contractions and possessives are left in (as 2 \"words\")\n",
    "        # due to the assumption that spaCy understands the relationship between them\n",
    "        \n",
    "        frequency[(text, tag)] += 1\n",
    "\n",
    "    #---Your code ends here\n",
    "    \n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwxEexOa8WRw"
   },
   "source": [
    "Let's make sure your function works by testing it on a short sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTK6ombZ8WRw",
    "outputId": "13969fd9-6a65-4080-cec0-bcdc1e6fc339"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('the', 'DET'): 2,\n",
       "         ('quick', 'ADJ'): 1,\n",
       "         ('brown', 'ADJ'): 1,\n",
       "         ('fox', 'NOUN'): 1,\n",
       "         ('jump', 'VERB'): 1,\n",
       "         ('over', 'ADP'): 1,\n",
       "         ('lazy', 'ADJ'): 1,\n",
       "         ('dog', 'NOUN'): 1})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C1:SanityCheck\n",
    "\n",
    "count_words(\"The quick brown fox jumps over the lazy dog.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcKwjRXY8WRw"
   },
   "source": [
    "__C2.__ _(8 pts)_ Next, define a function called `book_TDM(book_id, pos = True, lemma = True)` and copy into it the TDM-producing code from __Section 2.1.5.1__ of the lecture notes, now `return`-ing `TDM` and `all_words`. Once copied, modify this function to call `count_words` appropriately, now passing through the user of `book_TDM`'s specified `lemma` and `pos` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vcan2bFq8WRx"
   },
   "outputs": [],
   "source": [
    "# C2:Function(8/8)\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def book_TDM(book_id, pos = True, lemma = True):\n",
    "\n",
    "    #---Your code starts here---\n",
    "    ids = str(book_id)\n",
    "    text_file = open(\"./data/books/\"+ids+\".txt\",\"r\")\n",
    "    text = text_file.read()\n",
    "\n",
    "    words = nlp(text)\n",
    "\n",
    "    all_words = set()\n",
    "    all_frequencies = {}\n",
    "\n",
    "    for j, sentence in enumerate(words.sents):\n",
    "      frequency = count_words(sentence.text,pos,lemma)\n",
    "      all_frequencies[j] = frequency\n",
    "      doc_words = set(frequency.keys())\n",
    "      all_words = all_words.union(doc_words)\n",
    "\n",
    "    TDM = np.zeros((len(all_words),len(all_frequencies)))  \n",
    "\n",
    "    all_words = sorted(list(all_words))\n",
    "\n",
    "    for j in all_frequencies:\n",
    "      for i, word in enumerate(all_words):\n",
    "        TDM[i,j] = all_frequencies[j][word]\n",
    "    #---Your code ends here---\n",
    "\n",
    "    return(TDM, all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4pvmaCH8WRx"
   },
   "source": [
    "\n",
    "To test your code's function, let's process `book_id = 84` with both of `pos = True` and `lemma = True` and print out the `TDM`'s `.shape` attribute and the first ten terms in `all_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dc-lQol48WRx",
    "outputId": "76daf593-b38d-4fa4-8ce8-b38942ff1e3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'s\", 'PART'),\n",
       " ('1', 'NUM'),\n",
       " ('10', 'NUM'),\n",
       " ('11', 'NUM'),\n",
       " ('11th', 'ADJ'),\n",
       " ('11th', 'NOUN'),\n",
       " ('12', 'NUM'),\n",
       " ('12th', 'NOUN'),\n",
       " ('13', 'NUM'),\n",
       " ('13th', 'NOUN')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C2:SanityCheck\n",
    "\n",
    "TDM, terms = book_TDM(\"84\", pos = True, lemma = True)\n",
    "terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LrAWUQ7N8WRx",
    "outputId": "be8fd4ac-4f45-4002-88a5-2f924bd4c562"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6265, 3281)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C2:SanityCheck\n",
    "\n",
    "TDM.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndsa3WNZ8WRy"
   },
   "source": [
    "__C3.__ _(8 pts)_ Next, your job is to define two functions. The first is `sim(u,v)`, which shoud take two arbitrary numeric vectors and compute/output the `cosine_similarity`, as described in __Section 1.1.2.10__.  \n",
    "\n",
    "The second function is `term_sims(i, TDM)`, which should utilize the first function (`sim` function) to output a list of cosine similarity values (`sim_values`) between the word/row `i` and all others (rows) in the `TDM`.\n",
    "\n",
    "Note: each of these functions can be straightforwardly completed using a single line of code! Exhibit your knowledge of comprehensions and vectorization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CHHW5ISB8WRy"
   },
   "outputs": [],
   "source": [
    "# C3:Function(4/8)\n",
    "def sim(u,v):\n",
    "\n",
    "    \n",
    "    #---Your code starts here\n",
    "\n",
    "    cosine_similarity = u.dot(v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "    #---Your code ends here\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16aLBHKs8WRy",
    "outputId": "be001054-f41d-4984-f40d-2d6a02d9a689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactly similar: 1.0\n",
      "Exactly dissimilar: -1.0\n",
      "In the middle: 0.0\n"
     ]
    }
   ],
   "source": [
    "# C3:SanityCheck\n",
    "\n",
    "print(\"Exactly similar:\", sim(np.array([1,2,3]), np.array([1,2,3])))\n",
    "print(\"Exactly dissimilar:\", sim(np.array([1,2,3]), np.array([-1,-2,-3])))\n",
    "print(\"In the middle:\", sim(np.array([1,1]), np.array([-1,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jQ2zrsrf8WRy"
   },
   "outputs": [],
   "source": [
    "# C3:Function(4/8)\n",
    "\n",
    "def term_sims(i, TDM):\n",
    "    \n",
    "    #---Your code starts here\n",
    "    \n",
    "    sim_values = [sim(TDM[i,], TDM[x,])\n",
    "    for x in range(TDM.shape[0]) if x!= i]\n",
    "\n",
    "    #---Your code ends here\n",
    "    \n",
    "    return sim_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SnyHOcc88WRz",
    "outputId": "385cbde6-b3bf-41d6-be4c-7ef1f20702ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05986843400892498,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07332355751067664,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14664711502135327,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.11757927025044308,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.20739033894608505,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05986843400892498,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04016096644512494,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07332355751067664,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.07332355751067664,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05986843400892498,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1555427542095638,\n",
       " 0.09525009525014289,\n",
       " 0.022628141110071023,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07332355751067664,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04313979814072881,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04067259173910421,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.01728252824550709,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.045256282220142045,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.20739033894608505,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10621916186265284,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.026128726722320684,\n",
       " 0.0,\n",
       " 0.028759865427152538,\n",
       " 0.03456505649101418,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04233337566673017,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03666177875533832,\n",
       " 0.07332355751067664,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.020739033894608506,\n",
       " 0.05184758473652126,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05184758473652126,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07332355751067664,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.028759865427152538,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.07332355751067664,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05986843400892498,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.20739033894608505,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05986843400892498,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1796053020267749,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.019956144669641657,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.20739033894608505,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.18549555830406728,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.07332355751067664,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.10369516947304253,\n",
       " 0.10369516947304253,\n",
       " 0.032791291789197645,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05986843400892498,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.11575971249124357,\n",
       " 0.04637388957601682,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.06100888760865632,\n",
       " 0.06253053994807224,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.07332355751067664,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.06632365324280728,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07332355751067664,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.022107884414269093,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.08466675133346034,\n",
       " 0.07332355751067664,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05029954548278569,\n",
       " 0.0,\n",
       " 0.03126526997403612,\n",
       " 0.0,\n",
       " 0.05986843400892498,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09379580992210836,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.08314109932105399,\n",
       " 0.05986843400892498,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07332355751067664,\n",
       " 0.0,\n",
       " 0.04637388957601682,\n",
       " 0.0,\n",
       " 0.04233337566673017,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.11757927025044308,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.029329423004270657,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05144721129508729,\n",
       " 0.05184758473652126,\n",
       " 0.03126526997403612,\n",
       " 0.0,\n",
       " 0.05029954548278569,\n",
       " 0.031916699728542886,\n",
       " 0.044215768828538185,\n",
       " 0.0777713771047819,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05776712868787002,\n",
       " 0.0,\n",
       " 0.04233337566673017,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03126526997403612,\n",
       " 0.01818933598994668,\n",
       " 0.036919057552173014,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.01778357485073947,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07332355751067664,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.016395645894598822,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03126526997403612,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04637388957601682,\n",
       " 0.03666177875533832,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03666177875533832,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.06852369676512932,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07838618016696206,\n",
       " 0.0,\n",
       " 0.13818056002487877,\n",
       " 0.0,\n",
       " 0.03666177875533832,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.01778357485073947,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.011666618055859373,\n",
       " 0.03126526997403612,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.032791291789197645,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07332355751067664,\n",
       " 0.03225806451612903,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04233337566673017,\n",
       " 0.0,\n",
       " 0.021166687833365085,\n",
       " 0.07332355751067664,\n",
       " 0.0,\n",
       " 0.05986843400892498,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.027713699773684663,\n",
       " 0.028759865427152538,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03666177875533832,\n",
       " 0.015813365095036138,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.034113206256964415,\n",
       " 0.015813365095036138,\n",
       " 0.04032589923951654,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03991228933928331,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.025149772741392844,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07418005900049589,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.02318694478800841,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10168147934776052,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03456505649101418,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.07332355751067664,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05184758473652126,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.015289018630918963,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.032791291789197645,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03919309008348103,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10943584600739437,\n",
       " 0.014243626957572003,\n",
       " 0.09274777915203364,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03786412228313766,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.025723605647543644,\n",
       " 0.016395645894598822,\n",
       " 0.04744009528510841,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03320903209244786,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10369516947304253,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.012763993017791393,\n",
       " 0.0,\n",
       " 0.01398225379936557,\n",
       " 0.0,\n",
       " 0.05184758473652126,\n",
       " 0.022107884414269093,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03919309008348103,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03919309008348103,\n",
       " 0.11085479909473865,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.06913011298202835,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03786412228313766,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04637388957601682,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C3:SanityCheck\n",
    "\n",
    "# Compare word/row 0 to all other (rows) in the TDM\n",
    "term_sims(0, TDM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pCvalbr8WRz"
   },
   "source": [
    "__C4.__ _(7 pts)_ Finally, your goal now is to a write function, `most_similar(term, terms, TDM, top = 25)`, that utilizes `term_sims` to output a sorted list of the `top = N` terms (`top_n_terms`) most similar to one specified (`term`). The output data type should be a list of lists, with each inner list representing information for a similar term as: `[row_ix, similarity, term]`. \n",
    "\n",
    "\\[Hint: to locate the row containing the term of interest, utilize the list `.index()` method in application to the `terms` argument.\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zmoAiZLQ8WRz"
   },
   "outputs": [],
   "source": [
    "# C4:Function(6/7)\n",
    "\n",
    "def most_similar(term, terms, TDM, top = 25):\n",
    "    \n",
    "    #---Your code starts here---\n",
    "    \n",
    "    disc = []\n",
    "    i = terms.index(term)\n",
    "    func = term_sims(i,TDM)\n",
    "    for j, eachsim in enumerate(func):\n",
    "        disc.append([j, eachsim, terms[j]])\n",
    "    sim_data = sorted(disc,key = lambda x: x[1], reverse = True)\n",
    "    top_n_terms = sim_data[:top]\n",
    "    \n",
    "    #---Your code ends here---\n",
    "    \n",
    "    return top_n_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rX6beiwh8WRz"
   },
   "source": [
    "Now, let's test your functions utility on a `TDM` produced for `book_id = 84` and exhibit the top 25 similar terms to both of `('monster', 'NOUN')` and `('beautiful', 'ADJ')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfVzrGxP8WR0",
    "outputId": "21ddd877-07b6-41ee-c43b-8da9fb47b753"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[39, 0.17407765595569785, ('Abhorred', 'PROPN')],\n",
       " [638, 0.17407765595569785, ('aloud', 'ADJ')],\n",
       " [804, 0.17407765595569785, ('asseveration', 'NOUN')],\n",
       " [830, 0.17407765595569785, ('attached', 'ADJ')],\n",
       " [994, 0.17407765595569785, ('besiege', 'VERB')],\n",
       " [1019, 0.17407765595569785, ('blameless', 'ADJ')],\n",
       " [1538, 0.17407765595569785, ('convulse', 'VERB')],\n",
       " [1881, 0.17407765595569785, ('dim', 'NOUN')],\n",
       " [1937, 0.17407765595569785, ('disown', 'VERB')],\n",
       " [2524, 0.17407765595569785, ('flight', 'VERB')],\n",
       " [2572, 0.17407765595569785, ('forehead', 'NOUN')],\n",
       " [2662, 0.17407765595569785, ('furiously', 'ADV')],\n",
       " [3180, 0.17407765595569785, ('inevitable', 'ADJ')],\n",
       " [3366, 0.17407765595569785, ('jeer', 'VERB')],\n",
       " [3439, 0.17407765595569785, ('languish', 'VERB')],\n",
       " [3503, 0.17407765595569785, ('lid', 'NOUN')],\n",
       " [3707, 0.17407765595569785, ('merciless', 'PROPN')],\n",
       " [3748, 0.17407765595569785, ('mirror', 'NOUN')],\n",
       " [3804, 0.17407765595569785, ('mortal', 'NOUN')],\n",
       " [4482, 0.17407765595569785, ('proportionably', 'ADV')],\n",
       " [4573, 0.17407765595569785, ('rational', 'ADJ')],\n",
       " [4947, 0.17407765595569785, ('scream', 'NOUN')],\n",
       " [5773, 0.17407765595569785, ('unallied', 'ADJ')],\n",
       " [6066, 0.17407765595569785, ('watery', 'ADJ')],\n",
       " [6142, 0.1484539238050411, ('wholly', 'ADV')]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C4:SanityCheck\n",
    "\n",
    "most_similar(('monster', 'NOUN'), terms, TDM, top = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ez_Kwaya8WR0",
    "outputId": "2c3097a7-538e-4bc1-8715-054b829dd451"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4425, 0.26726124191242434, ('pride', 'NOUN')],\n",
       " [4746, 0.26726124191242434, ('research', 'NOUN')],\n",
       " [24, 0.1889822365046136, ('27th', 'NOUN')],\n",
       " [225, 0.1889822365046136, ('La', 'PROPN')],\n",
       " [229, 0.1889822365046136, ('Lavenza', 'PROPN')],\n",
       " [273, 0.1889822365046136, ('Montalegre', 'PROPN')],\n",
       " [310, 0.1889822365046136, ('Pays', 'PROPN')],\n",
       " [406, 0.1889822365046136, ('Uri', 'PROPN')],\n",
       " [407, 0.1889822365046136, ('Valais', 'PROPN')],\n",
       " [408, 0.1889822365046136, ('Vaud', 'PROPN')],\n",
       " [412, 0.1889822365046136, ('Villa', 'PROPN')],\n",
       " [572, 0.1889822365046136, ('affright', 'NOUN')],\n",
       " [629, 0.1889822365046136, ('alluring', 'ADJ')],\n",
       " [657, 0.1889822365046136, ('amid', 'ADP')],\n",
       " [854, 0.1889822365046136, ('aught', 'NOUN')],\n",
       " [920, 0.1889822365046136, ('bat', 'NOUN')],\n",
       " [1165, 0.1889822365046136, ('candour', 'NOUN')],\n",
       " [1337, 0.1889822365046136, ('coast', 'NOUN')],\n",
       " [1601, 0.1889822365046136, ('critical', 'ADJ')],\n",
       " [1658, 0.1889822365046136, ('dart', 'VERB')],\n",
       " [1672, 0.1889822365046136, ('dazzle', 'VERB')],\n",
       " [1674, 0.1889822365046136, ('de', 'NOUN')],\n",
       " [2086, 0.1889822365046136, ('ecstasy', 'VERB')],\n",
       " [2102, 0.1889822365046136, ('elapse', 'VERB')],\n",
       " [2135, 0.1889822365046136, ('empty', 'ADJ')]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C4:SanityCheck\n",
    "\n",
    "most_similar(('beautiful', 'ADJ'), terms, TDM, top = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0qJBZqi08WR0",
    "outputId": "54fa424d-2d0b-473b-c9a1-31ddcf906a06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "# C4:Inline\n",
    "\n",
    "# Comment on the ordered results returned in the sanity checks.\n",
    "# Do you think the algorithm is exhibiting sensible results? print \"Yes\" or \"No\"\n",
    "print(\"Yes\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "A1-module-C.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
